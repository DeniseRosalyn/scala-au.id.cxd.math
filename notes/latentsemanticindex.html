<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Scala-au.id.cxd.math by cxd</title>

    <link rel="stylesheet" href="../stylesheets/styles.css">
    <link rel="stylesheet" href="../stylesheets/github-light.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/idea.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>

    <meta name="viewport" content="width=device-width">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>
<body>
<div class="wrapper">
    <p>
        <a href="../index.html">Home</a>
    </p>
    <h2>
        <a id="welcome-to-github-pages" class="anchor" href="#welcome" aria-hidden="true"><span
                aria-hidden="true" class="octicon octicon-link"></span></a>
        Notes on Singular Value Decomposition and Latent Semantic Indexing
    </h2>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
"HTML-CSS": {preferredFont: 'TeX'}
});


    </script>
    <script type="text/javascript" async
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
    <div>
        <p>
            The implementation of Latent Semantic Indexing in this package is motivated primarily as an application of
            singular value decomposition
            to term document vectors built out each entry in a corpus of short documents. The resulting vectors form the
            row of a term weight matrix where
            the rows represent a document and the columns represent the terms extracted from the entire corpus. Once the
            matrix is formed the singular value decomposition
            is applied to obtain the left hand eigenvectors the singular values and the right hand eigenvectors.
        </p>
        <p>
            It is intended as an exploration of the application of the SVD to
        <ul>
            <li>Provide an understanding of the amount of entropy within a data set.</li>
            <li>Review the contribution of each component to the total variation within the data set.</li>
            <li>Reduce the number of dimensions in the data set using the matrices resulting from the decomposition. The
                reduced form can be used as an input to other processes.
            </li>
            <li>Provide a mapping into a geographic representation which can be used for projections of search queries
                in order to retrieve objects (documents) that are close in proximity to the search query via a distance
                metric.
            </li>
            <li>Provide a means of automatically categorising documents (objects) through assigning each document to the
                maximally weighted component.
            </li>
            <li>Provide a means of automatically categorising terms (attributes) through assigning each attribute to the
                maximally weighted component.
            </li>
            <li>Provide smoothed estimates for the original data, and provide estimates for the correlation matrix of
                documents (objects) and the correlation matrix of terms (attributes).
            </li>
            <li>Provide a method of visualising high dimensional multivariate-data through the use of the resulting
                component vectors.
            </li>
        </ul>


        </p>
        <p>
            Note that the technique of matrix decomposition is useful not only as a preprocessing step, the ability to project the original feature vectors of
            the data set into the principle components (as derived from either the left or right hand components) as a geometric
            space permits a convenient method of searching which may rely on distance measures between two vectors, the query vector, and each vector in the
            resulting search space. Similar techniques have been used in both early methods of search within signal processing such as for versions of facial recognition
            (as in eigenfaces) and also as the search step in voice biometrics, and the geometric projection remains in use during the search stage of current algorithms.
            </p>
        <p>
            The application to Latent Semantic Indexing gives an example of how
            the decomposition can be applied for such search methods, however when applied to different domains the initial treatment of the data will differ.
            For example, in the case of text processing, feature vectors may be constructed out of different techniques for mapping documents into
            weighted term vectors. Whereas for applications in voice biometrics for example, the feature space will be derived from the key features identified
            in processing the voice signal, which is necessarily a frequency based domain. For the early examples of facial biometrics, such as eigenfaces, the
            features are derived from a normalised collection of face images. Once the model is constructed it may be referred to as a background model for example,
            new records may be added by projecting the feature vector into the reduced component space, and the resulting record may be added as a row in the U matrix.
            Note however the number of components in the reduced space does not change, only the number of objects is increased.</p>
        <p> In the case of latent semantic indexing there is a limitation where the model is limited by the vocabularly that is learnt from the original training set.
            Adding new documents can be achieved by encoding the document into the term document vector that contains the same number of terms
            as the original corpus. The resulting vector can then be projected into the search space and added as an object to the U matrix.
            However, if new terms are desired, then it would in general be necessary to recompute the entire term document matrix, and in turn recompute the SVD
            and reduce the dimensions of the SVD accordingly.
        </p>
        <p>
            The implementation provides a starting point for exploring those areas listed, and further experimentation.
        </p>
        <h3>Singular Value Decomposition</h3>
        <p>

            The singular value decomposition provides a decomposition of a matrix such that
        </p>
        $$
        A = USV'
        $$
        <p>
            The decomposition is described in Skillicorn 2007, as one which indicates the amount of variation in the
            latent features of the data set.

            If $A$ is a $m \times n$ matrix, the number of components $k \leq n$ and where practical $k \leq m$.
        <ul>
            <li> $U$ will be a matrix of the dimension $m \times k$ representing the eigenvalues for the rows of the
                matrix $A$.
            </li>
            <li>$S$ will be a vector of dimension $k$ representing the eigenvalues for the matrix $A$</li>
            <li>$V'$ will be a matrix of dimension $k \times n$ representing the eigenvalues of the columns of the
                matrix $A$.
            </li>
        </ul>

        Several interpretations are explained in the book giving the following views
        </p>
        <h5>Factor interpretation</h5>
        <p>
            The rows of $V'$ are interpreted as underlying factors. The transformed basis provides a set of new factors
            a correlation between the original attributes of A and the rows of V can be constructed in order to
            determine
            the relationship between the original attributes and the factors in V. This relationship may include
            positive and
            negative relationships for multiple attributes with a single factor in $V'$.
        </p>

        <h5>Geometric interpretation</h5>
        <p>
            The interpretation of the components is similar to the interpretation of PCA in this approach.
            When reviewing the relationship of of objects to attributes the $V'$ matrix can be seen as an orthonormal
            basis for the matrix $U$
            Alternately, $U$ also represents an orthonormal basis for the matrix $V'$.
        </p>
        <p>
            This is especially useful for visualisation. For example, coordinates from $U$ can be plotted,
            these are separated by the directions of the corresponding components in $V'$.
        </p>

        <h5>Graphical interpretation</h5>
        <p>
            The resulting matrices describe the following
            The matrix $U$ describes the relationship between objects or rows of A, the transpose matrix $V'$ describes
            the
            relationship between attributes or columns in $A$ with the rows of $V'$ or columns of $V$ its transpose.
            The diagonal (or vector) $S$ describes the amount of variation in the respective components which are the
            columns of $U$
            and the columns of $V'$.
        </p>
        <p>
            A useful difference between PCA and SVD is that whilst PCA is capable of measuring either the relationship
            between objects $(A'A)$
            or the relationship between attributes $AA'$ it can only do so distinctly, and not both at the same time in
            the single operation.
            Whilst the SVD can provide the measurements of components for objects and attributes in one operation.
        </p>
        <p>
            Data is considered to be standardised and spherically distributed prior to executing the procedure, due to
            this
            an appropriate method of normalisation or standardisation should be performed on the input data set prior to
            performing the procedure.
        </p>
        <p>
            There are some exceptions to this rule as in the case of matrices of dummy variables for example.

            The $S$ matrix in the components interpretation provides a measure of the amount of variation in each component.

            Note that in the Breeze implementation if the <pre>P.cols > P.rows</pre> the SVD will calculate the number of
            components equal to the <pre>min(P.rows, P.cols)</pre>
        </p>

        <h3>Latent Semantic Indexing</h3>
        <p>
            Latent Semantic Indexing is an example of the application of the SVD to a corpus of natural language documents where the
            components collapse the similar term features from the document corpus into a single component [2]. The technique starts by
            first encoding each document as a feature vector, which results first from tokenising, preprocessing and finally weighting terms.
        </p>
        <p>
            There are a number of issues to consider in preprocessing natural text. This implementation applies a simplistic approach where each document
            is tokenised using spaces or punctuation marks via a regular expression, and the resulting tokens are passed through a porter stemmer
            (<a href="https://en.wikipedia.org/wiki/Stemming">https://en.wikipedia.org/wiki/Stemming</a>).
            Different term weighting schemes available, however the approach to term weighting in the implementation uses the TFIDF term weighting scheme (
            <a href="https://en.wikipedia.org/wiki/Tf–idf">https://en.wikipedia.org/wiki/Tf–idf</a>).
        </p>
        <p>
          The TFIDF weighting is applied once all documents have been tokenised and stemmed and the entire vocabulary of terms has been identified. The terms
            are then hashed using the built-in hashcode of java, and ordered based on the hashcode. This then defines the order of the columns for the feature vectors
            of each document. Once this order is determined, the TF-IDF is then computed for each document as follows.
            <br/>
            For each term, the term frequency for a given document is calculated using the augmented frequency to prevent bias towards longer documents
            $$
            tf(t, d) = 0.5 + 0.5 \times \frac{f_{t,d}}{max\left(f_{t',d} : t' \in d\right)}
            $$
            where $f_{t,d}$ is the frequency of the term for the given document $d$.<br/>
            The inversse document frequency it is calculated by retaining of total count of documents that contain each term, and uses the logarithm of
            $$
            idf(t, D) = \log{\left[ \frac{N}{\left| d \in D : t \in d \right|} \right]}
            $$
            Once the $idf$ is calculated then the final weight is defined as:
            $$
              tf-idf(t,d) = tf(t,d) \times idf(t,D)
            $$
        </p>
        <p>
            Once the TF-IDF matrix $A$ is computed the SVD is then calculated
            $$
              \hat{A} = U S V'
            $$
            and can be used for both exploration of the corpus and analysis of the components. In the example implementation the text corpus is
            taken from the Apache projects bug database, where the documents correspond to the titles of the bugs, the corpus has over 5000 examples of issues raised.

        </p>
        <h4>Entropy and Variation explained by Components</h4>
        <p>
            Initially after performing the SVD, it is possible to calculate the entropy of the data set which indicates how much noise exists in the data
            and provides a heuristic measure that can be used to determine how the components explain the amount of variation.
        In order to calculate the entropy, the percent of the contribution $f_k$ of each singular value in $S$ is calculated
            $$
            f_k = s_k&#94;2/\sum_{i=1}&#94;r s_i&#94;2
            $$
            These values can be plotted as a scree plot, which provides an indication as to the importance of each component. Note that the original
            set of singular values $S$ are listed in descending order, so too the percentage of variation explained.
            The total entropy for $r$ components is then calculated
            $$
               \entropy = \frac{\sum_{k=1}&#94;r f_k \log{f_k}}{\log{r}}
            $$
        </p>



        <h5>References:</h5>

        <ol>
            <li>Skillicorn, D. Understanding Complex Datasets: Data Mining with Matrix Decompositions. Chapman and
                Hall/CRC 2007
            </li>
            <li>Manning, Christopher D. Schutze, Hinrich. Foundations of Statistical Natural Language Processing. Massachusetts Institute of Technology 1999</li>
        </ol>


    </div>

</div>
<script src="javascripts/scale.fix.js"></script>

</body>
</html>