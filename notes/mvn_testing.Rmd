---
title: "Notes on Testing for Multivariate Normality"
output: 
  html_document:
    highlight: pygments
    css: ['../stylesheets/styles.css','../stylesheets/github-light.css']
    includes:
      in_header: mvn_testing_head.html
csl: journal-of-combinatorics.csl
bibliography: bibliography.bib
---

<a href="../index.html">Home</a>

It is necessary to check the assumption of multivariate normality prior to applying procedures which have the assumption of MVN, and common variance. For example, prior to applying MANOVA, procedures such as PCA or linear regression.

There are several methods available for checking for multivariate normality, which include visual inspection of Mahalanobis distance values for each observation from the expected chisq quantile, as well as performing tests available such as the Mardia, Henze-Zirkler and Royston tests which each use different measures to test for multivariate normality.

## Example Plotting Mahalanobis Distance over Chisq Quantiles

One visual method of inspecting multivariate data for normality is to plot the mahalanobis distances obtained over the quantiles of the chisq distribution with 2 degrees of freedom.

The library implements the Mahalanobis distance measure via:

$$
dist(X) = \left\{ (X-\bar{X})' S_{X,X}^{-1} (X-\bar{X}) \right\}^{1/2}
$$

Where $S_{X,X}^{-1}$ is the inverse of the estimated variance-covariance matrix and $\bar{X}$ is the estimated column mean vector for the matrix X.


```{r, echo=FALSE}
library(rscala)
library(knitr)
cp <- getwd()
includes <- list.files(file.path(cp, "../target/scala-2.12"), full.names=TRUE)
# ... args passed to rscala::scala functions. See ?rscala::scala for more informations.
make_scala_engine <- function(...) {
 
  engine <- scala(serialize.output = TRUE, stdout = "", JARs=includes)
  engine <- force(engine)
  function(options) {
    code <- paste(options$code, collapse = "\n")
    output <- capture.output(invisible(engine + code))
    engine_output(options, options$code, output)
  }
}

# Register new engine in knitr
knit_engines$set(scala = make_scala_engine())

```


```{r, engine='scala', echo=FALSE, message=FALSE, results='hide'}
import java.io._
import breeze.linalg._

def toCsv[A](path:String, items:Seq[A]) {
  val writer = new PrintWriter(new File(path))
  val data = items.mkString("\n")
  writer.write(data)
  writer.flush()
  writer.close()
}

def writeMatCsv(path:String, m:DenseMatrix[Double]):Unit = {
  val file = new java.io.File(path)
  csvwrite(file, m, separator=',')
}
```


```{r, engine='scala', echo=FALSE}
val basePath ="/Users/cd/Projects/scala/scala-au.id.cxd.math-gh-pages/scala-au.id.cxd.math"
```

The visual assessment can be demonstrated on two data sets. The first example is a subset of the iris data set, subsetted on the species "virginica", in order to generate an example of a multivariate normal data sample.


```{r, engine='scala', message=FALSE, results='hide'}
// imports
import java.io.File
import scala.collection.mutable
import au.id.cxd.math.data.CsvReader

import au.id.cxd.math.data.MatrixReader
import au.id.cxd.math.function.transform.StandardisedNormalisation
import au.id.cxd.math.probability.analysis._
import breeze.linalg.{DenseMatrix, eigSym, inv, svd}

import au.id.cxd.math.function.distance.MahalanobisDistance
import au.id.cxd.math.probability.continuous.ChiSquare



val fileName:String = s"$basePath/data/iris_virginica.csv"

val mat = MatrixReader.readFileAt(fileName)
// extract the columns of interest.
val data = mat(::, 0 to 3)
```

Once the data is loaded we then obtain the distance measures for each observation.

```{r, engine='scala', message=FALSE, results='hide'}
// get the distance measures for each example.
val distance = MahalanobisDistance(data)

// sort distance from lowest to highest.
val distSorted = distance.toArray.sorted

// chisquare quantiles
val n = distSorted.length
val x = (for (i <- 1.0 to n.toDouble by 1.0) yield (i-0.5)/n.toDouble).toArray
val df = data.cols
val chisq = ChiSquare(df)
val quantiles = x.map(chisq.invcdf(_))

```

Now we will produce a plot of the ordered distance measure over the chisq quantiles.


```{r, engine='scala', message=FALSE, results='hide', echo=FALSE}

toCsv(s"$basePath/temp/distSorted.csv", distSorted)
toCsv(s"$basePath/temp/x.csv", x)
toCsv(s"$basePath/temp/quantiles.csv", quantiles)

```

```{r, echo=FALSE}
x <- c(t(read.csv("../temp/x.csv", header=FALSE)))
distances <- c(t(read.csv("../temp/distSorted.csv", header=FALSE)))
quantiles <-c(t(read.csv("../temp/quantiles.csv", header=FALSE)))
```

```{r cleaned, results="asis"}
distances <- sapply(distances, function(d) d^2)

m <- lm(quantiles ~ distances)
intercept <- m$coefficients[1]
slope <- m$coefficients[2]

plot(distances, quantiles, col="blue", main="QQPlot of squared mahalanobis distance over Chisq Quantiles df=4")
abline(0,1, col="red")

```

The plot above suggests the data is close to multivariate normal, since the squared mahalanobis distance closely follows the chisquared distribution of the same degree of freedom, with 4 attributes. 

We can also repeat this process using R for comparison.

```{r}
data1 <- read.csv("../data/iris_virginica.csv", header=TRUE)
data <- (data1[,1:4])
mu <- colMeans(data)
sigma <- cov(data)
dist <- mahalanobis(data, mu, sigma)


df <- ncol(data)
n <- length(dist)
u <- ((1:n)-0.5)/n
p <- qchisq(u,df)
distsorted <- sort(dist)

plot(distsorted,p, 
     col="blue",
     main="QQ Plot of mahalnobis distance v chisq quantiles")
abline(0,1, col="red")

```


### A QQPlot for non-MVN distributed data

The mandible data set is not multivariate normally distributed, as the groups do not share common variance (as shown in the [manova](manova.html) notes).

The qqplot can be displayed in a similar manner using the Mahalanobis distance measures.

```{r, engine='scala', message=FALSE, results='hide'}


val fileName2:String = s"$basePath/data/test_mandible_data.csv"

val dataRange:Seq[Int] = 2 to 10

val mat2 = MatrixReader.readFileAt(fileName2)
val m2 = mat2(::, dataRange).toDenseMatrix
val data2 = StandardisedNormalisation().transform(m2)

val distance2 = MahalanobisDistance(data2)

// sort distance from lowest to highest.
val distSorted2 = distance2.toArray.sorted

// chisquare quantiles
val n2 = distSorted2.length
val x2 = (for (i <- 1.0 to n2.toDouble by 1.0) yield (i-0.5)/n2.toDouble).toArray
val df2 = data2.cols
val chisq2 = ChiSquare(df2)
val quantiles2 = x2.map(chisq2.invcdf(_))

```


```{r, engine='scala', message=FALSE, results='hide', echo=FALSE}

toCsv(s"$basePath/temp/distSorted2.csv", distSorted2)
toCsv(s"$basePath/temp/x2.csv", x2)
toCsv(s"$basePath/temp/quantiles2.csv", quantiles2)

```

```{r, echo=FALSE}
x <- c(t(read.csv("../temp/x2.csv", header=FALSE)))
distances2 <- c(t(read.csv("../temp/distSorted2.csv", header=FALSE)))
quantiles2 <-c(t(read.csv("../temp/quantiles2.csv", header=FALSE)))
```

Displaying the plot below

```{r results="asis"}

distances2 <- sapply(distances2, function(d) d^2)

plot(distances2, quantiles2, col="blue", main="QQPlot of mandible data")
abline(0,1, col="red")

```

It is apparent that there are a small number of examples whose distances deviate from the expected quantiles. It may be interesting to identify which observations they are and when the segments (species of dog in this case) are known before hand identify whether there may be some commonality of segment or not for those cases. It appears that the other samples may correspond closely to observations from an MVN distribution, it is possible that the four examples that deviate largely could be outliers or particularly interesting observations.

## Tests for Multivariate Normality.

There are a number of procedures available for testing multivariate normality. The procedures are summarised in the article by Kormaz, Goksuluk and Zarasiz @korkmaz.

The procedures include

- The Mardia Test
- The Henze-Zirkler Test
- The Royston Test

When inspecting data for multivariate normality, it is advisable to use the combination of tests and visualisation, as well as using more than one test in order to determine whether the procedures agree.

The __Mardia Test__ makes use of statistics for skew and kurtosis.

Like the Mahalanobis distance measure $m$, the kurtosis and skew statistics are derived from the matrix

$$
m^2 = R = (X-\bar{X})'S^{-1}(X-\bar{X})
$$

Where we have the estimate of the parameters for the MVN distribution, the mean vector parameter $\bar{X}$ and the estimate of the variance covariance matrix $S$.

The measures of skewness $b_{1,p}$ and kurtosis $b_{2,p}$ are calculated as
 
$$
b_{1,p} = \frac{1}{n^2} \sum_{i,j=1}^n r^3_{ij}
$$
and
$$
b_{2,p} = \frac{1}{n}\sum_{i=1}^n r^2_{ii}
$$
where $n$ is the number of observations in the sample $X$

For large $n$ the test statistic for skewness is derived as
$$
z_1 = \frac{n}{6}b_{1,p}
$$
which has a $\chi^2$ distribution with df
$$
df = p(p+1)(p+2)/6
$$

and the statistic for kurtosis $z_2$ is
$$
z_2 = \sqrt{n} \frac{b_{2,p} -  p(p+2) } { \sqrt{8 p(p+2) } }
$$
which is distributed as $N(0,1)$. It is a standardisation of $b_{2,p}$ with $\mu=p(p+2)$ and $\sigma = \sqrt{ 8p(p+2)/n}$

In the case of small sample sizes less then 20 observations, a correction factor $c$ is given for $z_1$
$$
c = \frac{(n+1)(n+3)(k+1)}{n(n+1)(k+1)-6}
$$
then $z1 = \frac{nc}{6}b_{1,p}$.

The null hypothesis asserts that the data is MVN distributed. Two tests are performed using the statistics and associated distributions. In order to reject the null hypothesis, either of the tests must disagree, both tests must agree in order to accept the MVN hypothesis.


```{r, engine='scala', message=FALSE, results='hide'}
import au.id.cxd.math.probability.analysis.MardiaTest

// test with standardised data
val X = StandardisedNormalisation().transform(data)
val test = MardiaTest(0.05, X)
```

Printing the result:

```{r, engine='scala'}
println(test.toString)
```

As both tests do not reject the null hypothesis, the outcome suggests that the set of observations are from a multivariate normal distribution.


The Mardia Test from the MVN package is demonstrated below for the iris subset of data from above. The results in the mardia test require at least one of the kurtosis and skew test results to disagree in order to for the null hypothesis to be rejected. 

```{r, message=FALSE, error=FALSE, warning=FALSE}
require(MVN)
mvn(data, mvnTest="mardia", multivariatePlot="qq")
```

## An example of testing non-MVN data.

Similarly we can observe the result for the non-multivariate data for the mandible dataset.

```{r, engine='scala', message=FALSE, results='hide'}
// test with standardised data
val test2 = MardiaTest(0.05, data2)
```

Printing the results;

```{r, engine='scala'}
println(test2)
```

We can see that the test rejects the null hypothesis of MVN for this particulare dataset.

For comparison in R as below,

```{r}
data2 <- read.csv("../data/test_mandible_data.csv", header=TRUE)
mvn(data2[,1:9], mvnTest="mardia")

```

reinforces the conclusion that the dataset is not multivariate normal.


## References

