<!DOCTYPE html >
<html>
        <head>
          <title>sequence - au.id.cxd.math.model.sequence</title>
          <meta name="description" content="sequence - au.id.cxd.math.model.sequence" />
          <meta name="keywords" content="sequence au.id.cxd.math.model.sequence" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../../../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../../../../../lib/jquery.js" id="jquery-js"></script>
      <script type="text/javascript" src="../../../../../../lib/jquery-ui.js"></script>
      <script type="text/javascript" src="../../../../../../lib/template.js"></script>
      <script type="text/javascript" src="../../../../../../lib/tools.tooltip.js"></script>
      
      <script type="text/javascript">
         if(top === self) {
            var url = '../../../../../../index.html';
            var hash = 'au.id.cxd.math.model.sequence.package';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="value">
      <div id="definition">
        <img alt="Package" src="../../../../../../lib/package_big.png" />
        <p id="owner"><a href="../../../../../package.html" class="extype" name="au">au</a>.<a href="../../../../package.html" class="extype" name="au.id">id</a>.<a href="../../../package.html" class="extype" name="au.id.cxd">cxd</a>.<a href="../../package.html" class="extype" name="au.id.cxd.math">math</a>.<a href="../package.html" class="extype" name="au.id.cxd.math.model">model</a></p>
        <h1>sequence</h1><span class="permalink">
      <a href="../../../../../../index.html#au.id.cxd.math.model.sequence.package" title="Permalink" target="_top">
        <img src="../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">sequence</span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        
        
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="au.id.cxd.math.model.sequence.HiddenMarkovModel" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="HiddenMarkovModel"></a>
      <a id="HiddenMarkovModel:HiddenMarkovModel"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="HiddenMarkovModel$.html"><span class="name">HiddenMarkovModel</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../index.html#au.id.cxd.math.model.sequence.package@HiddenMarkovModel" title="Permalink" target="_top">
        <img src="../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt"><script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
"HTML-CSS": {preferredFont: 'TeX'}
});
</script>
<script type="text/javascript" async src="https://cxd.github.io/scala-au.id.cxd.math/javascripts/MathJax-2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
 </p><div class="fullcomment"><div class="comment cmt"><p><script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
"HTML-CSS": {preferredFont: 'TeX'}
});
</script>
<script type="text/javascript" async src="https://cxd.github.io/scala-au.id.cxd.math/javascripts/MathJax-2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
 </p><p>The HMM is calculated out of two processes, a forward and backward process.</p><p>The model itself consists of two matrices and a prior matrix.</p><p>A state transition matrix $A$ of $M \times M$ representing the likelihood of transitioning
to state $X_{t+1}$ at time $t+1$ from the previous state $X_t$ at time $t$.
The values $a_{ij}$ represent the transition from $X_t$ to $X_{t+1}$ hence $i = t$ and $j = t + 1$.
The table forms the likelihood $P(X_{t+1}|X_t)$ such that</p><p>$$
a_{ij} = P(x_j | x_i)
$$</p><p>The second matrix $B$ represents a state evidence transition matrix. Given $M$ states there are $N$
evidence variables, hence $B$ is an $M \times N$ matrix where
$$
b_{ij} = P(x_j | e_i)
$$</p><p>Additionally there is a prior probability for each state $X$ provided by a vector $\pi$ of length $M$ where</p><p>$$
\pi_i = P(x_i)
$$</p><p>The Hidden Markov model comprises of</p><p>$$
HMM = \lambda = (A, B, \pi)
$$</p><p>The procedure of learning the hidden markov model is to seek to learn the posterior matrices for $B$
given an initial condition. This implementation addresses the discrete model, rather than the continuous model
and seeks to learn the matrix $A$ relating state $X$ to the other states $x$ and the matrix $B$ relating
the state $X$ to the evidence variable $e$ give the stream of previous events in time $1 : t$ it seeks to
use the posterior model to predice the next state at time $t + k + 1$ and the prior states from the
sequential model in time $t + k$.</p><p>$$
P(X_{t+k+1}|e_{1:t}) = \sum_{x_t+k} P(X_{t+k+1}|x_{t+k})P(x_{t+k}|e_{1:t})
$$</p><p>Note that both $A$ and $B$ are as the likelihood model, and $\pi$ is initialised as the prior.</p><p>This is estimated from the training data. The raw format of the training data is a jagged array
which consists of a</p><pre><span class="std">List</span>[<span class="std">List</span>[<span class="std">String</span>]]</pre><p>Where the last item in the list represents a discrete state label for the domain.
Each item preceding that represents a discrete token for an evidence label in the domain.</p><p>So each row in the jagged array contains a variable sequence of evidence labels and terminates with a state label
which is emitted by the sequence.</p><p>Both the domain of evidence labels and state labels are deterministic in this model.
For example, if we have a simple weather world and we observe two evidence variables:</p><pre>Umbrella, NoUmbrella</pre><p>And we have two state variables,</p><pre>Raining,Sunny</pre><p>We can describe a sequence that leads up to Raining as</p><pre>Umbrella,Raining</pre><p>Or</p><pre>Umbrella,Umbrella,Raining</pre><p>We may potentially have a sequence that terminates in Sunny as</p><pre>Umbrella,Umbrella,NoUmbrella,Sunny</pre><p>For example.</p><p>The matrix $A$ is estimated by identifying the sequences of length $n$ and $n+1$ and then counting the frequency
of the transition between $x_n$ and $x_{n+1}$.</p><p>$$
a_{ij} = \frac{c_{ij}}{\sum_k c_{ik}}
$$</p><p>The evidence matrix $B$ is estimated by identifying the frequency of $b_{m}$ terminating in $x_{n+1}$
for rows in the traning data $M$ and terminal states in $N$ lines ending in state $x_n$.</p><p>$$
b_{ij} = \frac{ c(e_i, x_j) }{ \sum_k c(e_i, x_k) }
$$</p><p>The class <a href="../../data/SequenceEstimation.html" class="extype" name="au.id.cxd.math.data.SequenceEstimation">au.id.cxd.math.data.SequenceEstimation</a> is used in this library to estimate these original likelihoods
and the prior distribution for $X$.</p><p>$$
\pi_i = \frac{c_i}{\sum_k c_{ik}}
$$</p><p><br/></p><p>The procedure for learning is performed using a &quot;forward backward&quot; algorithm, there are two stages.</p><p>The backward variable represents the probability that the model is in state $x_i(t)$ at time $t$.</p><p>A matrix $\beta$ is defined to store the transition model at time $t$ such that</p><p>$$
\beta_i(t) = 0 \text{ if } x_i \ne x_0 \text{ and } t = T
$$
$$
\beta_i(t) = 1 \text{ if } x_i = x_0 \text{ and } t = T
$$
$$
\beta_i(t) = \sum_j^N a_{ij}b_j(e_{t+1})\beta_j(t + 1) \text{ otherwise }
$$</p><p>this matrix represents the probability that the sequence of evidence variables seen up until time $T$
from $1:t \rightarrow T$ will generate the state $x_i$</p><p>$$
\beta_i(t) = P(e_{t+1}, e_{t+2}, ..., e_T | x_i, \lambda)
$$</p><p>The second part of the algorithm is to compute the forward variable $\alpha$ a matrix that is defined
to represent the probability that the model is in state $x_i$ given the sequence of evidence variables
observed so far.</p><p>$$
\alpha_j(t) = P(e_1, e_2, ..., e_t, x_i | \lambda)
$$</p><p>it is defined as follows</p><p>$$
\alpha_j(t) = 0 \text{ if } t = 0 \text{ and } x_j \ne x_0
$$
$$
\alpha_j(t) = 1 \text{ if } t = 0 \text{ and } x_j = x_0
$$
$$
\alpha_j(t) = \left[\sum_i^N \alpha_i(t-1)a_{ij}\right]b_j(e_t) \text{ otherwise }
$$</p><p>as both are recursive procedures a dynamic programming technique is required to compute the values.</p><p>The model for $A$ is iteratively updated for each transition at time $t$ and $t + 1$ by estimating the
probability of transition from state $x_i$ to state $x_j$ at time $t$ and $t+1$ given the current model $\lambda$
and the set of evidence variables $V_{t+1} = e_1,e_2,e_3,...,e_{t+1}$ the new estimates are stored
in a matrix $\epsilon$.</p><p>The equation for $\epsilon_{ij}$ is given as:</p><p>$$
\epsilon_{ij} = P(x_i(t), x_j(t+1)|V_{t+1}, \lambda)
$$</p><p>$$
\frac{\alpha_i(t)b_j(e_{t+1})\beta_j(t+1)}{P(V_{t+1}|\lambda)}
$$</p><p>$$
\frac{\alpha_i(t)b_j(e_{t+1})\beta_j(t+1)}{\sum_{i=1} \sum_{j=1} \alpha_i(t)a_{ij}b_j(e_{t+1})\beta_j(t+1)}
$$</p><p>The expected number of times that state $x_i$ transitions to state $x_j$ is given by summing over the values
of $\epsilon_{ij}$.</p><p>$$
\gamma_i(t) = \sum_{j=1}^T \epsilon_t(i,j)
$$</p><p>The total number of expected times a state $x_i$ is visited is given by summing over $\gamma_i$ for all time $T$.</p><p>$$
\sum_{t=1}^T \gamma_i(t)
$$</p><p>The matrix $A$ can be updated with the new estimates for the state transitions as follows:</p><p>$$
\hat{a_{ij}} = \frac{\gamma_i(t)}{\sum_{i=1}^T \gamma_i(t)}
$$</p><p>The matrix $B$ can be updated with new estimates for the evidence variables and states by calculating
the ratio between the frequency that a particular evidence variable $e_k$ is produced and any evidence variable is produced.</p><p>$$
\hat{b_{ij}} = \frac{\sum_{t=1,e_k} \gamma_k(t) }{\sum_{t=1} \gamma_i(t)}
$$</p><p>The learing process repeats until convergence where the changes in the previous values of
$a_{ij}$ and $b_{ij}$ decrease below a threshold $\theta$.</p><p>When working with multiple sequences $B_k$ the sequences are assumed to be independent of each other
and a normalisation factor is introduced in the estimate of $\hat{A}$.</p><p>$$
c = \sum_{k=1}^K \frac{1}{P_k}
$$</p><p>which is the marginal distribution of observation sequence k from the set of observation sequences
$O = e_1,...,e_n$ for each of the sequences in the input sample b. Assuming independence</p><p>$$
P(O|\lambda) = \prod_{k=1} P(O_k|\lambda)
$$</p><p>The update rules for $\hat{A}$ and $\hat{B}$ are changed as follows.</p><p>$$
\bar{a_{ij}} = \frac{c \sum_{t=1} \alpha_{k,i}(t)b_j(e_{k,t+1})\beta_{k,j}(t+1)}{c \sum_{t=1} \alpha_{k,i}(t)\beta_{k,i}(t+1)}
$$</p><p>$$
\bar{b_{ij}} = \frac{c \sum_{t=1,e_j} \alpha_{k,i}(t)\beta_{k,ij}(t) }{c \sum_{k=1} \alpha_{k,i}(t)\beta_{k,i}(t) }
$$</p><p>the summations above are from $t=1$ to $T-1$ in both numerator and denominator.</p><p><u>Example Usage</u></p><p>The library is used in combination with the <a href="../../data/SequenceReader.html" class="extype" name="au.id.cxd.math.data.SequenceReader">au.id.cxd.math.data.SequenceReader</a> and
<a href="../../data/SequenceEstimation.html" class="extype" name="au.id.cxd.math.data.SequenceEstimation">au.id.cxd.math.data.SequenceEstimation</a> classes the purpose of these classes are to
read data from CSV and then generate the initial likelihood for matrices $A$, $B$ and
the prior vector $\pi$.</p><p>The test class &quot;TestTrainRainModel&quot; in the test cases for the project provides an example
of the usage and the class TestRainExample provides an example of the data format. See also
the example in TestTrainCtiModel for the use of CSV training data.</p><p>The classes are used to create a <a href="../entity/hmm/InputModel.html" class="extype" name="au.id.cxd.math.model.entity.hmm.InputModel">au.id.cxd.math.model.entity.hmm.InputModel</a>
which contains the initialisation parameters and are fed to the hmm algorithm.</p><p>Intiialising the model for example:</p><pre><span class="kw">val</span> fileName = <span class="lit">"data/example_train_data.csv"</span>
<span class="kw">val</span> file = <span class="kw">new</span> File(fileName)
<span class="kw">val</span> reader = SequenceReader()
<span class="cmt">// the data set (jagged array)</span>
<span class="kw">val</span> data = reader.readSequences(file)
<span class="cmt">// unique set of states</span>
<span class="kw">val</span> states =  reader.readStates (data)
<span class="cmt">// the unique labels for evidence variables</span>
<span class="kw">val</span> evidenceVars = reader.readEvidenceVars (data)</pre><p>After which the sequence estimator is used to generate the initial parameters of the model.</p><pre><span class="kw">val</span> estimator = SequenceEstimation()

<span class="kw">val</span> pi = estimator.statePriors(data)(states)

<span class="kw">val</span> A = estimator.stateTransitions(data)(states)

<span class="kw">val</span> Bk = estimator.avgPriorEvidences(data)(evidenceVars)(states)

<span class="kw">val</span> input = InputModel(pi, A, <span class="std">List</span>(Bk), states, evidenceVars)</pre><p>The input model is then used to train the HMM</p><pre><span class="kw">val</span> model = HiddenMarkovModel.train(input)(data)(<span class="num">0.00001</span>)(<span class="num">50</span>)</pre><p>The parameters include the threshold for convergence and maximum iterations.</p><p>Once trained the model can be used for prediction.</p><p>The example below is from the &quot;CTI&quot; world where telephony events result in a call state.
Refer also to the TestTrainRainModel for the weather world example.</p><pre><span class="kw">val</span> test = <span class="std">List</span>(<span class="lit">"Ringing(inbound)"</span>,
       <span class="lit">"UserEvent(Start)"</span>,
       <span class="lit">"UserEvent(Stop)"</span>,
       <span class="lit">"OffHook"</span>,
       <span class="lit">"Established"</span>,
       <span class="lit">"Held"</span>,
       <span class="lit">"Dialing(Consult)"</span>);

     <span class="kw">val</span> predict1 = HiddenMarkovModel.viterbiPredict (model) (test)</pre><p>The string of events that are sent to the predict model do not include the end state.</p><p>The prediction model will return a result for each evidence transition in the input chain.</p><p>For example, from the input above, each step in the sequence above is categorised
with a corresponding state label, and the probability of that state label.</p><pre>  <span class="std">List</span>({
        prob: <span class="num">6.60507635396296E-44</span>,
        state: OnCall
        evidence: Ringing(inbound)
        T: <span class="num">1</span>
        success: <span class="kw">true</span>
}, {
        prob: <span class="num">7.758309697332233E-70</span>,
        state: Started
        evidence: UserEvent(Start)
        T: <span class="num">2</span>
        success: <span class="kw">true</span>
}, {
        prob: <span class="num">3.2302220856424765E-94</span>,
        state: Paused
        evidence: UserEvent(Stop)
        T: <span class="num">3</span>
        success: <span class="kw">true</span>
}, {
        prob: <span class="num">1.6782307523456403E-112</span>,
        state: OnCall
        evidence: OffHook
        T: <span class="num">4</span>
        success: <span class="kw">true</span>
}, {
        prob: <span class="num">1.1980508062616898E-131</span>,
        state: OnCall
        evidence: Established
        T: <span class="num">5</span>
        success: <span class="kw">true</span>
}, {
        prob: <span class="num">1.0950613538482917E-145</span>,
        state: OnHold
        evidence: Held
        T: <span class="num">6</span>
        success: <span class="kw">true</span>
}, {
        prob: <span class="num">1.0950613538482917E-145</span>,
        state: Consult
        evidence: Dialing(Consult)
        T: <span class="num">7</span>
        success: <span class="kw">true</span>
})</pre><p>For further reading refer to:</p><p>Norvig, Peter. Russell, Stuart. (2003) $Artificial\ Intelligence\ A\ Modern\ Approach.\ Second\ Edition.$ USA: Pearson Education, Inc. pp537-583, 724-733.</p><p>Bishop, Christopher M. (2006) $Pattern\ Recognition\ and\ Machine\ Learning$ USA: Springer Science + Business Media, LLC. pp607-635.</p><p>Rabiner, Lawrence R. (1989) <code><code>A Tutorial on Hidden Markov Models and Seleted Applications in Speech Recognition<i>. USA: Proceedings of the IEEE, VOL 77, NO.2 pp257-286</i></code></code></p><p>Created by cd on 10/01/15.
</p></div></div>
    </li><li name="au.id.cxd.math.model.sequence.SingularSpectrumAnalysis" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="SingularSpectrumAnalysis"></a>
      <a id="SingularSpectrumAnalysis:SingularSpectrumAnalysis"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="SingularSpectrumAnalysis$.html"><span class="name">SingularSpectrumAnalysis</span></a><span class="result"> extends <a href="../../function/series/WindowedMatrix.html" class="extype" name="au.id.cxd.math.function.series.WindowedMatrix">WindowedMatrix</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../index.html#au.id.cxd.math.model.sequence.package@SingularSpectrumAnalysis" title="Permalink" target="_top">
        <img src="../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">An implementation of the singular spectrum analysis (SSA)</p>
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>


    </body>
      </html>